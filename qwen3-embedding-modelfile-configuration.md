# Qwen3-Embedding Modelfile Configuration Guide

**Date**: November 23, 2025  
**Project**: KiloCode Codebase Indexing  
**Model**: qwen3-embedding:8b-fp16

---

## Executive Summary

**The default Ollama modelfile for qwen3-embedding:8b-fp16 requires NO modification.** Unlike text generation models, embedding models don't use parameters like temperature, top_p, or num_ctx. The minimal default modelfile is exactly what's needed for optimal performance.

---

## Understanding the Default Modelfile

When you run `ollama show qwen3-embedding:8b-fp16 --modelfile`, you see:

```
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM qwen3-embedding:8b-fp16

FROM /root/.ollama/models/blobs/sha256-9a2dfcc2e867828909456dd52a69e3775b677bdce1816f7cc55f3657393e7e53
TEMPLATE {{ .Prompt }}
```

**This is complete and correct.** Here's why:

### Component Breakdown

1. **FROM**: Points to the model weights blob
   - This is the actual FP16 quantized Qwen3-Embedding-8B model
   - ~15GB of trained neural network parameters

2. **TEMPLATE**: Defines input format
   - `{{ .Prompt }}` means "insert the input text here"
   - No special formatting needed for embeddings
   - Ollama handles Qwen3-specific tokens automatically

**That's it.** Nothing else is needed or beneficial to add.

---

## Why Embedding Models Are Different

### Text Generation Models (e.g., Llama, Qwen3-Instruct)

**Purpose**: Create new text by predicting the next token

**Key Parameters**:
```
PARAMETER temperature 0.8       # Controls randomness/creativity
PARAMETER num_ctx 4096          # Context window size for generation
PARAMETER top_p 0.9             # Nucleus sampling threshold
PARAMETER repeat_penalty 1.1    # Penalizes repetition
PARAMETER stop "<|endoftext|>"  # Stop sequences
```

**Process**: 
1. Takes input text
2. Generates probability distribution over next possible tokens
3. Samples from distribution (using temperature, top_p, etc.)
4. Repeats until stop condition or max length

### Embedding Models (e.g., qwen3-embedding)

**Purpose**: Convert text into fixed-size numerical vectors

**Key Parameters**:
```
(none - embeddings are deterministic)
```

**Process**:
1. Takes input text
2. Processes through neural network layers
3. Outputs fixed-size vector (e.g., 1024 dimensions)
4. Same input ALWAYS produces same output

**No sampling, no randomness, no generation** = no need for those parameters.

---

## What Ollama Handles Automatically

According to the official Qwen3-Embedding documentation, these models have specific requirements:

### Requirements (Handled by Ollama)

1. **Special Token Appending**
   - Qwen3-embedding needs `<|endoftext|>` appended to inputs
   - ✅ Ollama does this automatically via the `/api/embeddings` endpoint

2. **Output Normalization**
   - Embedding vectors must be normalized (L2 norm = 1)
   - ✅ Ollama handles this internally

3. **Pooling Strategy**
   - Uses "last token" pooling to create embeddings
   - ✅ Built into the model architecture

4. **Context Window**
   - Supports up to 32,000 tokens
   - ✅ Built into model, no configuration needed

### What This Means

You don't need to add anything to the modelfile to enable these features. Ollama's embedding API endpoint is specifically designed to handle embedding models correctly.

---

## What NOT to Add to the Modelfile

**Don't add these parameters** - they are irrelevant or counterproductive for embeddings:

```
# ❌ DON'T ADD THESE ❌

PARAMETER temperature 0.8
# Why: Embeddings are deterministic, no sampling occurs

PARAMETER num_ctx 32000
# Why: Context window is built into the model architecture

PARAMETER top_p 0.9
PARAMETER top_k 50
# Why: No probability distribution sampling happens

PARAMETER repeat_penalty 1.1
# Why: Not generating text, so no repetition to penalize

PARAMETER stop "<|endoftext|>"
# Why: Not generating sequences that need to stop

SYSTEM "You are a helpful assistant"
# Why: Embeddings don't have personalities or roles
```

Adding these parameters won't cause errors, but they'll be **completely ignored** by Ollama's embedding endpoint. They're visual clutter that serves no purpose.

---

## Context Window Details

### Built-In Capacity

Qwen3-Embedding-8B supports **32,000 tokens** of input context:
- ~25,000 words
- ~180,000 characters
- Entire chapters of books
- Large code files

### For KiloCode Codebase Indexing

Your KiloCode settings:
- **Min block size**: 100 characters
- **Max block size**: 1,000 characters

**Result**: You're using 0.5% of the available context window per code block. There's enormous headroom - you'll never hit the limit.

### Why You Don't Configure It

The 32K context window is hardcoded in the model's architecture:
```python
# Inside the model architecture (conceptual)
class Qwen3Embedding:
    def __init__(self):
        self.max_position_embeddings = 32768  # Built-in
        self.hidden_size = 4096
        # ... other architecture details
```

You can't change it via modelfile, and you don't need to - it's already optimal.

---

## Dimension Configuration (Matryoshka Support)

### Default Behavior

Qwen3-Embedding-8B outputs **1024 dimensions** by default when used through Ollama's API:

```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3-embedding:8b-fp16",
  "prompt": "test"
}'

# Returns: array of 1024 floats
```

### Why 1024 is Optimal

As detailed in the main setup report:
- 98-99% of maximum quality (vs 4096 dims)
- 4× less storage than full dimensions
- Faster search performance
- Industry standard for production code search

### Changing Dimensions (If Needed)

The model supports flexible dimensions (32-4096) via Matryoshka Representation Learning. However, **this is NOT configured in the modelfile**. Instead, you specify it in the API call:

```bash
# Request 512 dimensions instead
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3-embedding:8b-fp16",
  "prompt": "test",
  "options": {
    "num_embed": 512
  }
}'
```

**For KiloCode**: The dimension setting goes in KiloCode's configuration UI, not in the Ollama modelfile.

---

## Verification and Testing

### Test 1: Verify Model is Loaded

```bash
# Check model exists
docker exec ollama ollama list | grep qwen3

# Expected output:
# qwen3-embedding:8b-fp16    <hash>    15 GB    <timestamp>
```

### Test 2: Generate Test Embedding

```bash
# Generate embedding for a code snippet
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3-embedding:8b-fp16",
  "prompt": "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)"
}'
```

**Expected Response**:
```json
{
  "model": "qwen3-embedding:8b-fp16",
  "embeddings": [[
    0.0234, -0.0156, 0.0891, ..., 0.0445
  ]],
  "total_duration": 145231234,
  "load_duration": 1234567,
  "prompt_eval_count": 23
}
```

**Validation Checklist**:
- ✅ Response status: 200 OK
- ✅ Embeddings array has exactly 1024 numbers
- ✅ All numbers are floats between -1 and 1
- ✅ Total duration < 500ms (typical for single embedding)

### Test 3: Verify Determinism

```bash
# Generate same embedding twice
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3-embedding:8b-fp16",
  "prompt": "hello world"
}' > embedding1.json

curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3-embedding:8b-fp16",
  "prompt": "hello world"
}' > embedding2.json

# Compare (should be identical)
diff embedding1.json embedding2.json
```

**Expected**: No differences. Same input = same output, always.

### Test 4: Check VRAM Usage

```bash
# Monitor GPU memory
nvidia-smi

# With model loaded, expect:
# GPU 0 (RTX 4090): ~15GB used by Ollama
```

---

## KiloCode Configuration

### Settings to Use

In KiloCode's "Codebase Indexing" settings:

```yaml
Enable Codebase Indexing: ✅ ON

Embedding Provider:
  Provider: Ollama
  Base URL: http://localhost:11434
  Model: qwen3-embedding:8b-fp16    # ← Must match exactly
  Dimensions: 1024                   # ← Optional, auto-detected

Vector Database:
  Provider: Qdrant
  URL: http://localhost:6333
  API Key: (leave empty for local)
  Collection Name: kilocode_codebase
  
Search Settings:
  Max Search Results: 20
  Min Block Size: 100 chars
  Max Block Size: 1000 chars
```

### Critical Points

1. **Model Name Must Match**: `qwen3-embedding:8b-fp16`
   - KiloCode passes this exact string to Ollama
   - Case-sensitive
   - Must include the `:8b-fp16` tag

2. **Dimensions Are Auto-Detected**: 
   - KiloCode will query the model and detect 1024 dimensions
   - You can explicitly set to 1024 for clarity
   - Don't set to other values unless you know what you're doing

3. **No API Key Needed**:
   - Ollama runs locally without authentication
   - Only set API key if you've added authentication to Ollama (rare)

---

## Common Questions

### Q: Should I add `PARAMETER num_ctx 32000` to utilize the full context?

**A**: No. The 32K context window is already built into the model. Adding this parameter:
- Won't increase the context limit (already at max)
- Won't improve performance
- Has no effect on embedding generation

### Q: What about temperature for more "creative" embeddings?

**A**: This is a misunderstanding of what embeddings are. Embeddings are **deterministic mathematical transformations**, not creative generation. There's no creativity slider - the embedding of "hello" is always the same vector, by design.

### Q: Can I add a SYSTEM prompt to make embeddings better for code?

**A**: No. The model was trained specifically for code embeddings. Adding a system prompt:
- Has no effect (embeddings don't use prompts)
- Might confuse the API endpoint
- Is conceptually wrong - embeddings encode meaning, not follow instructions

### Q: Should I quantize the model further (Q8, Q4)?

**A**: You're already using FP16, which is the highest quality. Quantizing further:
- Saves VRAM (useful if you have <16GB)
- Reduces quality by ~0.5-4% depending on quantization
- For your hardware (24GB VRAM), FP16 is optimal

From your setup report, you chose FP16 specifically for maximum quality. Stick with it.

### Q: The modelfile looks "incomplete" compared to Llama models. Is something wrong?

**A**: No. Embedding models ARE simpler than generation models. Compare:

**Generation Model Modelfile** (~20 lines):
```
FROM llama3.1:8b
TEMPLATE """[INST] {{ .Prompt }} [/INST]"""
PARAMETER temperature 0.8
PARAMETER num_ctx 4096
PARAMETER stop "[INST]"
PARAMETER stop "[/INST]"
SYSTEM "You are a helpful assistant"
```

**Embedding Model Modelfile** (~3 lines):
```
FROM qwen3-embedding:8b-fp16
TEMPLATE {{ .Prompt }}
```

The embedding model modelfile is shorter because it does **one thing well**: convert text to vectors. It doesn't need all the generation control parameters.

---

## Troubleshooting

### Issue: "Model not found" error

**Solution**: Verify model name exactly matches
```bash
# Check available models
docker exec ollama ollama list

# Should see: qwen3-embedding:8b-fp16
```

If missing, pull again:
```bash
docker exec ollama ollama pull qwen3-embedding:8b-fp16
```

### Issue: Embedding dimension is not 1024

**Possible causes**:
1. Using a different tag (e.g., `:8b` instead of `:8b-fp16`)
2. Explicitly requesting different dimensions in API call
3. Using wrong model (e.g., `qwen3-embedding:4b`)

**Solution**: Verify with test embedding (Test 2 above)

### Issue: Very slow embedding generation (>1 second per embedding)

**Possible causes**:
1. Model not using GPU (offloading to CPU)
2. Other processes using VRAM
3. Model being loaded/unloaded repeatedly

**Diagnosis**:
```bash
# Check GPU usage during embedding generation
watch -n 1 nvidia-smi

# Check Ollama logs
docker logs ollama --tail 100 --follow
```

**Expected**: GPU utilization spike during embedding, ~50-100ms per embedding

### Issue: KiloCode can't connect to Ollama

**Common causes**:
1. Wrong URL (should be `http://localhost:11434`)
2. Ollama container not running
3. Port not mapped correctly

**Solution**:
```bash
# Check Ollama is running
docker ps | grep ollama

# Verify endpoint responds
curl http://localhost:11434/api/tags

# Should return list of models
```

---

## Summary and Best Practices

### ✅ Do This

1. **Use the default modelfile** - it's perfect as-is
2. **Test with curl** before configuring KiloCode
3. **Monitor VRAM** to ensure model fits comfortably
4. **Keep model name exact**: `qwen3-embedding:8b-fp16`
5. **Let Ollama handle** special tokens and normalization

### ❌ Don't Do This

1. **Don't add generation parameters** (temperature, top_p, etc.)
2. **Don't modify TEMPLATE** - keep it as `{{ .Prompt }}`
3. **Don't add SYSTEM prompts** - embeddings don't use them
4. **Don't try to set context window** - it's already 32K
5. **Don't overthink it** - simpler is better for embedding models

### Key Takeaway

**The "minimal" modelfile is actually the optimal configuration.** Embedding models are fundamentally simpler than generation models, and their modelfiles should reflect that simplicity. Trust the defaults, they're correct by design.

---

## References

- **Ollama Embedding Documentation**: https://ollama.com/blog/embedding-models
- **Ollama API Reference**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Qwen3-Embedding Official Docs**: https://ollama.com/library/qwen3-embedding
- **Project Setup Report**: `qwen3-embedding-kilocode-setup-report.md`
- **KiloCode Indexing Docs**: `kilocode-codebase-indexing-docs.md`

---

**Document Version**: 1.0  
**Last Updated**: November 23, 2025  
**Author**: AI Implementation Guide  
**Status**: Verified and Production-Ready
