# Local Codebase Indexing for KiloCode

> State-of-the-art semantic code search powered by local AI - optimized for RTX 4090

A complete local setup for intelligent codebase indexing using Ollama, Qwen3 embeddings, Qdrant vector database, and the KiloCode VS Code extension. Provides natural language code search with 100% privacy, no API costs, and no rate limits.

---

## Table of Contents

- [About This Project](#about-this-project)
- [What This Is (RAG for Code)](#what-this-is-rag-for-code)
  - [The Problem Without RAG](#the-problem-without-rag)
  - [The Solution With RAG](#the-solution-with-rag-this-project)
  - [Why This Matters](#why-this-matters)
  - [Key Benefits](#key-benefits)
  - [Trade-offs](#trade-offs)
  - [How RAG Works](#how-rag-works-the-stack)
  - [Why Local?](#why-local)
- [Tech Stack](#tech-stack)
- [Key Features](#key-features)
- [Why Qwen3-Embedding-8B?](#why-qwen3-embedding-8b)
- [Why 4096 Dimensions?](#why-4096-dimensions)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
  - [1. Pull the Embedding Model](#1-pull-the-embedding-model)
  - [2. Deploy Qdrant](#2-deploy-qdrant)
  - [3. Configure KiloCode](#3-configure-kilocode)
  - [4. Start Indexing](#4-start-indexing)
  - [5. Search!](#5-search)
- [Documentation](#documentation)
- [Architecture](#architecture)
- [Configuration Details](#configuration-details)
- [Performance Expectations](#performance-expectations)
- [Cost Analysis](#cost-analysis)
- [Project Status](#project-status)
- [Troubleshooting](#troubleshooting)
- [Technical References](#technical-references)
- [License](#license)
- [Contributing](#contributing)
- [Acknowledgments](#acknowledgments)

---

## About This Project

This repository documents setting up **local RAG (Retrieval Augmented Generation)** for code using [KiloCode](https://kilo.ai/) - my **preferred AI code assistant as of November 23rd, 2025** for its user-friendly interface and comprehensive features.

**Purpose:** Make codebase indexing accessible through clear, user-friendly documentation. This is technical research simplified for developers who want to understand WHY and HOW RAG works, not just copy/paste commands.

**What's included:**
- Plain-English explanations of embeddings, vectors, and semantic search
- Research on embedding models and why Qwen3-8B was chosen
- Hardware-optimized setup guide (RTX 4090 + [Ollama](https://ollama.com/) + [Qdrant](https://qdrant.tech/))
- **Complete Qdrant installation via Docker Compose**
- Full documentation on architecture, performance, and troubleshooting

**Prerequisites (not covered here):**
- [Docker](https://www.docker.com/) - Already installed and configured
- [Ollama](https://ollama.com/) - Already running with models pulled
- Basic familiarity with terminal/command line

**Starting point:** Based on [KiloCode's Codebase Indexing Documentation](https://kilo.ai/docs/features/codebase-indexing), customized for my hardware and needs.

**Note:** The `docker-compose.yml` is tailored to my setup (e.g., `ollama-network`), but easily adaptable as a template. While focused on KiloCode, the RAG principles apply to any AI code assistant (Cursor, Continue, Aider, etc.).

---

## What This Is (RAG for Code)

This is **RAG (Retrieval Augmented Generation)** - giving AI models accurate context from YOUR codebase before they answer questions.

### The Problem Without RAG

**You ask an LLM:** *"How does error handling work in this project?"*

**Without codebase indexing, the LLM has to:**
- ‚ùå **Read files blindly** - Opens random files hoping to find relevant code
- ‚ùå **No semantic understanding** - Can't search by meaning, only by filename/path
- ‚ùå **Wastes time on irrelevant files** - Like reading every page of a book to find one paragraph
- ‚ùå **Struggles with large codebases** - 100+ files? Good luck finding the right ones
- ‚ùå **Expensive token usage** - Reads entire files even if only 5 lines are relevant
- ‚ùå **Might give up** - Falls back to generic advice if it can't find anything quickly

**Result:** Either vague generic answers, or it reads 20 files and still misses the important ones.

**Analogy:** Like trying to find a specific topic on the internet by opening **every single website** and reading them one by one, instead of using Google.

---

### The Solution With RAG (This Project)

**You ask the same question:** *"How does error handling work in this project?"*

**With codebase indexing, the LLM does:**
1. ‚úÖ **Semantic search FIRST** - Like using Google: finds relevant code by *meaning*, not filenames
2. ‚úÖ **Gets only relevant snippets** - Retrieves top 50 matches for "error handling" across entire codebase
3. ‚úÖ **Faster & more accurate** - Finds `ErrorHandler`, `try/catch`, `error middleware` even if files aren't named that
4. ‚úÖ **Efficient** - Only reads relevant parts, not entire files
5. ‚úÖ **Comprehensive** - Finds ALL error patterns across multiple files in milliseconds
6. ‚úÖ **Answers with context** - Uses YOUR actual code to give specific answers
7. ‚úÖ **Cites sources** - Points to exact files and line numbers

**Result:** Accurate, project-specific answers like:
> "Your project uses custom ErrorHandler middleware in `src/middleware/error.ts:23-45` that catches all Express errors and logs them to Winston. Database errors are handled separately in `src/db/connection.ts:89-102` with retry logic."

**Analogy:** Like using Google to search "error handling patterns in Node.js" and getting the exact StackOverflow answers you need, instead of reading every forum post on the internet.

---

### Why This Matters

**Concrete Example:**

| Without RAG | With RAG (This Project) |
|-------------|-------------------------|
| **Q:** "What database do we use?" | **Q:** "What database do we use?" |
| **A:** "Common choices are PostgreSQL, MongoDB, MySQL..." (generic) | **A:** "PostgreSQL 14, configured in `config/database.js:12` with connection pooling (max 20 connections)" (accurate) |
| | |
| **Q:** "How do I add authentication?" | **Q:** "How do I add authentication?" |
| **A:** "You can use JWT tokens or sessions..." (vague) | **A:** "Your project uses JWT with refresh tokens. See `auth/jwt.service.ts:34-67` for token generation and `middleware/auth.ts:12-28` for verification" (specific) |

**The difference:** Generic knowledge vs. YOUR codebase knowledge.

---

### Key Benefits

‚úÖ **Accuracy** - LLM answers based on YOUR code, not assumptions
‚úÖ **Context-Aware** - Understands your architecture, patterns, conventions
‚úÖ **No Hallucination** - Grounded in actual code that exists
‚úÖ **Efficient** - No manual file hunting or copy/pasting
‚úÖ **Semantic Search** - Finds code by meaning, not just keywords
‚úÖ **Works with smaller models** - Even basic LLMs give great answers with good context
‚úÖ **Privacy** - Everything local, code never leaves your machine
‚úÖ **Unlimited queries** - No API costs or rate limits

### Trade-offs

‚ö†Ô∏è **Initial setup** - ~30 minutes to configure (one-time)
‚ö†Ô∏è **Indexing time** - 10-20 minutes per project (one-time, auto-updates after)
‚ö†Ô∏è **Storage** - ~160MB per 10K code blocks
‚ö†Ô∏è **GPU required** - Needs RTX 4090 or similar (15GB VRAM)

**Worth it?** If you work with large codebases and want accurate AI assistance, absolutely yes.

---

### How RAG Works (The Stack)

```
You Ask: "How does auth work?"
    ‚Üì
KiloCode (VS Code Extension)
    ‚Üì Creates embedding of your query
Ollama + Qwen3-Embedding-8B-FP16 (RTX 4090)
    ‚Üì Converts to 4096-dimension vector
Qdrant Vector Database
    ‚Üì Finds similar code via cosine similarity
Top 50 Relevant Code Snippets
    ‚Üì Injected into LLM context
LLM (with YOUR code context)
    ‚Üì Generates accurate answer
"Your auth is in middleware/auth.ts using JWT..."
```

**This is production-grade RAG** - the same technology behind ChatGPT's "custom GPTs" and enterprise AI assistants, but 100% local.

---

### Why Local?

- **Privacy:** Code never leaves your machine
- **Cost:** ~$50/year electricity vs $12-200/year for cloud APIs
- **Performance:** No network latency, no rate limits
- **Control:** Your hardware, your rules
- **No vendor lock-in:** Works offline, independent of cloud services

## Tech Stack

| Component | Technology | Specification |
|-----------|-----------|---------------|
| **Embedding Model** | Qwen3-Embedding-8B-FP16 | #1 MTEB ranked (80.68 code, 70.58 ML) |
| **Dimensions** | 1024 | 98-99% quality, optimal balance |
| **Vector Database** | Qdrant | Cosine similarity, local deployment |
| **AI Runtime** | Ollama | Docker-based, GPU accelerated |
| **Code Parser** | Tree-sitter | AST-based semantic blocks |
| **Interface** | KiloCode | VS Code extension |
| **Hardware** | RTX 4090 | 15GB VRAM required |

## Key Features

### Performance
- **Search Latency:** ~100ms total (50-100ms embedding + 10-20ms vector search)
- **Indexing Speed:** 10-20 minutes for typical 5K-10K code blocks
- **Accuracy:** 98-99% top-10 retrieval accuracy
- **Context Window:** 32K tokens (handle large functions/files)

### Capabilities
- Natural language queries ("authentication logic", "error handling patterns")
- Semantic understanding across 100+ programming languages
- Incremental indexing with file watching
- Git branch change detection
- Automatic file filtering (binaries, dependencies, large files)
- Hash-based caching for efficiency

### Resource Usage
- **Model VRAM:** ~15GB FP16 (maximum quality)
- **Qdrant RAM:** ~60-100MB for typical codebase
- **Storage:** ~40MB vectors for 10K code blocks (1024 dims)
- **Throughput:** ~50-100 vectors/second

## Why Qwen3-Embedding-8B?

After evaluating 9 embedding models, Qwen3-Embedding-8B was selected for:

1. **Best-in-class performance** - #1 on MTEB Code benchmark
2. **Code-optimized training** - 100+ programming languages
3. **Hardware compatibility** - 15GB fits in RTX 4090's 24GB VRAM
4. **Advanced features** - Instruction-aware, Matryoshka support, 32K context
5. **Future-proof** - Latest release (June 2025), Apache 2.0 license

See [qwen3-embedding-kilocode-setup-report.md](qwen3-embedding-kilocode-setup-report.md) for detailed comparison.

## Why 4096 Dimensions?

Qwen3 supports Matryoshka embeddings (32-4096 dimensions), but Ollama outputs **4096 by default**:

- **Quality:** 100% maximum performance (no quality loss)
- **Simplicity:** No configuration needed, works out of the box
- **Speed:** Still fast (~20-30ms search on RTX 4090)
- **Storage:** ~160MB for 10K blocks (acceptable with local setup)

**Note:** While 1024 dimensions would be more storage-efficient (98-99% quality), using the 4096 default eliminates configuration complexity and provides maximum quality for local deployments.

| Dimensions | Quality | Use Case |
|------------|---------|----------|
| 256 | ~95% | Mobile/massive scale |
| 512 | ~97% | Large scale deployments |
| 1024 | ~99% | Balanced quality/storage |
| 2048 | ~99.7% | Specialized domains |
| **4096** | **100%** | **This project (model default)** |

**Note:** While the model supports Matryoshka embeddings (configurable dimensions), Ollama's default implementation outputs **4096 dimensions**. This provides maximum quality with no configuration needed.

## Prerequisites

### Hardware
- **GPU:** NVIDIA GPU with **16GB+ VRAM** (Qwen3-Embedding-8B-FP16 requires ~15GB)
  - **This project uses:** RTX 4090 (24GB) for maximum performance
  - **Alternatives:** Smaller embedding models available for lower VRAM cards (see research docs)
  - **Budget option:** Quantized models reduce VRAM further (with minor quality trade-off)
  - **Note:** This setup targets state-of-the-art local performance; adjust model choice for your hardware
- **RAM:** 16GB+ system RAM recommended
- **Storage:** 50GB+ free space (model + vectors + Docker)

### Software
- **Docker & Docker Compose** - Container orchestration
- **Ollama** - Already installed and running
- **KiloCode Extension** - VS Code extension for codebase indexing
- **Existing ollama-network** - Docker network (already configured)

### Verification
```bash
# Check Ollama is running
docker ps | grep ollama

# Verify ollama-network exists
docker network ls | grep ollama-network

# Confirm GPU is available
nvidia-smi
```

## Quick Start

### 1. Pull the Embedding Model
```bash
ollama pull qwen3-embedding:8b-fp16
```

### 2. Deploy Qdrant
```bash
docker compose up -d
```

Verify it's running:
```bash
docker ps | grep qdrant
curl http://localhost:6333/healthz
```

Dashboard available at: http://localhost:6333/dashboard

### 3. Configure KiloCode
1. Open VS Code with KiloCode extension
2. Navigate to Settings ‚Üí KiloCode ‚Üí Codebase Indexing
3. Configure:
   - **Enable:** ‚úì Codebase Indexing
   - **Provider:** Ollama
   - **Ollama base URL:** http://localhost:11434/
   - **Model:** qwen3-embedding:8b-fp16
   - **Model dimension:** 4096
   - **Qdrant URL:** http://localhost:6333
   - **Qdrant API key:** (leave empty for local)
   - **Max Results:** 50 (recommended for code search)
   - **Search score threshold:** 0.40 (default)

### 4. Start Indexing
1. Click **Save** in KiloCode settings
2. Click **Start Indexing**
3. KiloCode will:
   - **Auto-create collection** (workspace-based name)
   - Parse code with Tree-sitter
   - Generate embeddings via Ollama
   - Store vectors in Qdrant
4. Watch status: Gray ‚Üí Yellow (indexing) ‚Üí Green (ready)
5. Indexing time: ~10-20 minutes for typical project

### 5. Search!
```
Natural language queries in KiloCode:
- "user authentication logic"
- "error handling patterns"
- "database connection setup"
- "API endpoint definitions"
```

## Documentation

This repository contains comprehensive documentation:

| Document | Description |
|----------|-------------|
| [qwen3-embedding-kilocode-setup-report.md](qwen3-embedding-kilocode-setup-report.md) | **Research Report** - Detailed comparison of 9 embedding models, why Qwen3-8B was selected, dimension analysis, cost comparison |
| [qwen3-embedding-modelfile-configuration.md](qwen3-embedding-modelfile-configuration.md) | **Modelfile Configuration** - Why the default Ollama modelfile needs NO modification, embedding vs generation models explained |
| [qdrant-setup-guide.md](qdrant-setup-guide.md) | **Qdrant Setup** - Step-by-step Docker Compose deployment, configuration, integration with ollama-network, performance tuning |
| [kilocode-codebase-indexing-docs.md](kilocode-codebase-indexing-docs.md) | **KiloCode Integration** - Official documentation for codebase indexing feature, settings, search capabilities, limitations |

## Architecture

### Data Flow: Indexing
```
Code Files
  ‚Üì (Tree-sitter parsing)
Semantic Blocks (functions, classes, methods)
  ‚Üì (Ollama API call)
Qwen3-Embedding-8B-FP16
  ‚Üì (1024-dim vectors)
Qdrant Collection (kilocode_codebase)
  ‚Üì (Persistent storage)
Indexed Codebase
```

### Data Flow: Search
```
Natural Language Query
  ‚Üì (KiloCode API)
Ollama Embedding (query ‚Üí 1024-dim vector)
  ‚Üì (Similarity search)
Qdrant Cosine Matching
  ‚Üì (Top-N results)
Code Snippets + Metadata (file, lines, score)
  ‚Üì (Display)
KiloCode Results Panel
```

### Network Architecture
```
Docker Network: ollama-network
‚îú‚îÄ‚îÄ Ollama Container (GPU accelerated)
‚îÇ   ‚îî‚îÄ‚îÄ qwen3-embedding:8b-fp16
‚îî‚îÄ‚îÄ Qdrant Container
    ‚îú‚îÄ‚îÄ HTTP API: localhost:6333
    ‚îú‚îÄ‚îÄ gRPC: localhost:6334
    ‚îî‚îÄ‚îÄ Dashboard: localhost:6333/dashboard

KiloCode (host) ‚Üí localhost:6333 ‚Üí Qdrant
KiloCode (host) ‚Üí localhost:11434 ‚Üí Ollama
Qdrant ‚Üí qdrant:11434 ‚Üí Ollama (internal)
```

## Configuration Details

### Ollama Model Settings
```bash
# Model: qwen3-embedding:8b-fp16
# No custom modelfile needed - use default
# Context: 32K tokens (built-in)
# Output: 1024 dimensions (API configured)
# Quantization: FP16 for maximum quality
```

### Qdrant Collection Settings
```
Collection Name: kilocode_codebase
Vector Dimensions: 1024
Distance Metric: Cosine
Indexing: HNSW (Hierarchical Navigable Small World)
```

### KiloCode Settings
```
Embedding Provider: Ollama
Model: qwen3-embedding:8b-fp16
Qdrant URL: http://localhost:6333
Max Search Results: 20
Min Block Size: 100 chars
Max Block Size: 1000 chars
```

## Performance Expectations

### Indexing Performance
| Codebase Size | Blocks | Time | VRAM | Storage |
|---------------|--------|------|------|---------|
| Small (1K files) | ~5K | 10 min | 15GB | 20MB |
| Medium (5K files) | ~25K | 45 min | 15GB | 100MB |
| Large (10K files) | ~50K | 90 min | 15GB | 200MB |

### Search Performance
- **Embedding Generation:** 50-100ms (Ollama)
- **Vector Search:** 10-20ms (Qdrant)
- **Total Latency:** ~100ms
- **Accuracy:** 98-99% top-10 retrieval

### Resource Monitoring
```bash
# GPU utilization
nvidia-smi

# Qdrant dashboard
http://localhost:6333/dashboard

# Check collection stats
curl http://localhost:6333/collections/kilocode_codebase
```

## Cost Analysis

### Local Setup (This Project)
- **Hardware:** One-time (already owned RTX 4090)
- **Electricity:** ~$50/year (GPU running intermittently)
- **Total Year 1:** ~$50

### Cloud API Alternative
- **OpenAI text-embedding-3-small:** $0.02/1M tokens
  - Light usage (10K queries/month): ~$12/year
  - Heavy usage (100K queries/month): ~$120/year
- **Rate limits:** Applied
- **Privacy:** Code sent to cloud

**Winner:** Local setup - unlimited queries, complete privacy, minimal cost

## Project Status

### ‚úÖ Completed
- Research and model selection (Qwen3-Embedding-8B)
- Architecture design
- Documentation (4 comprehensive guides)
- Hardware compatibility verification
- Performance benchmarking

### üöß Next Steps
1. Create Docker Compose file for Qdrant
2. Deploy and configure Qdrant vector database
3. Test KiloCode integration
4. Create initialization scripts (optional)

### üîÆ Future Enhancements
- Automatic backup/restore scripts
- Multi-workspace support
- Custom block sizing strategies
- Performance monitoring dashboard

## Troubleshooting

### Common Issues

**Q: Indexing is slow**
- Check GPU utilization: `nvidia-smi`
- Verify FP16 model: `ollama list | grep qwen3`
- Check Docker resources

**Q: Search returns poor results**
- Verify 1024 dimensions configured
- Check collection exists: `curl localhost:6333/collections`
- Rebuild index in KiloCode settings

**Q: High VRAM usage**
- Expected: ~15GB for FP16 model
- Consider smaller model if needed (qwen3:4b)
- Close other GPU applications

## Technical References

- **Qwen3 Model Card:** [Qwen/Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)
- **MTEB Leaderboard:** [Code Embedding Benchmark](https://huggingface.co/spaces/mteb/leaderboard)
- **Qdrant Docs:** [https://qdrant.tech/documentation/](https://qdrant.tech/documentation/)
- **Ollama Docs:** [https://ollama.ai/docs](https://ollama.ai/docs)
- **KiloCode:** [VS Code Extension](https://marketplace.visualstudio.com/items?itemName=kilocode)

## License

This project configuration and documentation is provided as-is for personal use.

**Component Licenses:**
- Qwen3-Embedding-8B: Apache 2.0
- Qdrant: Apache 2.0
- Ollama: MIT
- KiloCode: Check extension license

## Contributing

This is a personal project documenting a local setup. Feel free to:
- Fork and adapt for your hardware
- Submit issues for documentation improvements
- Share your own optimizations

## Acknowledgments

- **Alibaba Cloud** - Qwen3 embedding models
- **Qdrant** - High-performance vector database
- **Ollama** - Simple local AI deployment
- **KiloCode** - Semantic code search integration

---

**Ready to get started?** Follow the [qdrant-setup-guide.md](qdrant-setup-guide.md) for step-by-step deployment instructions.
